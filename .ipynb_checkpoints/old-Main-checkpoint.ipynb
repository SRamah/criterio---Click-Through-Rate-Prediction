{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDRegressor,SGDClassifier\n",
    "import zipfile\n",
    "import tarfile\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cleanNull(DF):\n",
    "    for column in DF.columns.values:\n",
    "        replace = DF[column].value_counts()\n",
    "        index_min = np.argmin(replace)\n",
    "        DF[column].fillna(index_min , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateFeatureEngine(DF , toGenerate):\n",
    "    for g in toGenerate :\n",
    "        for i in toGenerate :\n",
    "            if i != g :\n",
    "                newName = str(g) + \":\" + str(i)\n",
    "                DF[newName] = newName+DF[i].astype('str')+\":\"+DF[g].astype('str')\n",
    "                print newName\n",
    "                for j in toGenerate :\n",
    "                    if j!= i and j!=g :\n",
    "                        newX = newName + \":\" + str(j)\n",
    "                        DF[newX] = newX+DF[newName]+\":\"+DF[j].astype('str')\n",
    "                        print newX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DF = pd.read_csv(\"dac/4M_train.txt\", header=-1, sep='\\t')\n",
    "DF = pd.DataFrame(DF)\n",
    "num_Cols=[]\n",
    "cat_Cols = []\n",
    "dd = {}\n",
    "dd[0]=\"Label\"\n",
    "for i in range(1,14):\n",
    "    dd[i]=\"I\"+`i`\n",
    "    num_Cols.append(\"I\"+`i`)\n",
    "for c in range(1,27):\n",
    "    dd[c+13]=\"C\"+`c`\n",
    "    cat_Cols.append(\"C\"+`c`)\n",
    "DF.rename(columns=dd, inplace=True)\n",
    "#%time cleanNull(DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C10:C7\n",
      "C10:C7:C11\n",
      "C10:C11\n",
      "C10:C11:C7\n",
      "C7:C10\n",
      "C7:C10:C11\n",
      "C7:C11\n",
      "C7:C11:C10\n",
      "C11:C10\n",
      "C11:C10:C7\n",
      "C11:C7\n",
      "C11:C7:C10\n",
      "CPU times: user 24.7 s, sys: 16.3 s, total: 41 s\n",
      "Wall time: 46.4 s\n"
     ]
    }
   ],
   "source": [
    "TOGEN = ['C10', 'C7' , 'C11']\n",
    "%time generateFeatureEngine(DF,TOGEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 11s, sys: 37.1 s, total: 3min 48s\n",
      "Wall time: 6min 49s\n"
     ]
    }
   ],
   "source": [
    "%time DF.to_csv(\"FinalTrain.csv\" , header = False , index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Li = ['C10:C7','C10:C7:C11','C10:C11','C10:C11:C7','C7:C10','C7:C10:C11','C7:C11','C7:C11:C10','C11:C10','C11:C10:C7','C11:C7','C11:C7:C10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def DumpObject(name, Object):\n",
    "    # enregistrement ...\n",
    "    with open(name, 'wb') as fichier:\n",
    "        mon_pickler = pickle.Pickler(fichier)\n",
    "        mon_pickler.dump(Object)\n",
    "\n",
    "def LoadObject(name):\n",
    "    # Lecture des objets contenus dans le fichier...\n",
    "    with open(name, 'rb') as fichier:\n",
    "        mon_depickler = pickle.Unpickler(fichier)\n",
    "        object_recupere = mon_depickler.load()\n",
    "    return object_recupere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-01-05 01:16:24.887831\t Line : 500000\t logloss: 0.516480\t batch logloss: 0.516480\n",
      "2017-01-05 01:18:40.462875\t Line : 1000000\t logloss: 0.507712\t batch logloss: 0.498944\n",
      "2017-01-05 01:20:53.032654\t Line : 1500000\t logloss: 0.501929\t batch logloss: 0.490363\n",
      "2017-01-05 01:23:09.784855\t Line : 2000000\t logloss: 0.497722\t batch logloss: 0.485102\n",
      "2017-01-05 01:25:25.323747\t Line : 2500000\t logloss: 0.494996\t batch logloss: 0.484089\n",
      "2017-01-05 01:27:41.382943\t Line : 3000000\t logloss: 0.493411\t batch logloss: 0.485487\n",
      "2017-01-05 01:29:59.001894\t Line : 3500000\t logloss: 0.492438\t batch logloss: 0.486603\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from csv import DictReader\n",
    "from math import exp, log, sqrt\n",
    "from sklearn.utils import murmurhash3_32\n",
    "\n",
    "cat_Cols = []\n",
    "num_Cols=[]\n",
    "dd = {}\n",
    "dd[0]=\"Label\"\n",
    "for i in range(1,14):\n",
    "    dd[i]=\"I\"+`i`\n",
    "    num_Cols.append(\"I\"+`i`)\n",
    "for c in range(1,27):\n",
    "    dd[c+13]=\"C\"+`c`\n",
    "    cat_Cols.append(\"C\"+`c`)\n",
    "\n",
    "train = \"FinalTrain.csv\"\n",
    "header = [\"Label\"]\n",
    "header = header+num_Cols+cat_Cols+Li\n",
    "##############################\n",
    "\n",
    "logbatch = 500000\n",
    "D = 2 ** 22    # number of weights use for learning\n",
    "alpha = .004  # learning rate for sgd optimization\n",
    "\n",
    "# function definitions #######################################################\n",
    "\n",
    "# A. Bounded logloss\n",
    "# INPUT:\n",
    "#     p: our prediction\n",
    "#     y: real answer\n",
    "# OUTPUT\n",
    "#     logarithmic loss of p given y\n",
    "def logloss(p, y):\n",
    "    p = max(min(p, 1. - 10e-17), 10e-17)        # The bounds\n",
    "    return -log(p) if y == 1. else -log(1. - p)\n",
    "\n",
    "\n",
    "# B. Apply hash trick of the original csv row\n",
    "# for simplicity, we treat both integer and categorical features as categorical\n",
    "# INPUT:\n",
    "#     csv_row: a csv dictionary, ex: {'Lable': '1', 'I1': '357', 'I2': '', ...}\n",
    "#     D: the max index that we can hash to\n",
    "# OUTPUT:\n",
    "#     x: a list of indices that its value is 1\n",
    "def get_x(csv_row, D):\n",
    "    fullind = []\n",
    "    x = {}\n",
    "    for key, value in csv_row.items():\n",
    "        s = key + '=' + value\n",
    "        fullind.append(murmurhash3_32(s, positive=True) % D)\n",
    "        \n",
    "    \n",
    "    x[0] = 1  # 0 is the index of the bias term\n",
    "    for index in fullind:\n",
    "        if(not x.has_key(index)):\n",
    "            x[index] = 0\n",
    "        x[index] += 1\n",
    "\n",
    "        \n",
    "    return x # x contains indices of features that have a value as number of occurences\n",
    "\n",
    "\n",
    "# C. Get probability estimation on x\n",
    "# INPUT:\n",
    "#     x: features\n",
    "#     w: weights\n",
    "# OUTPUT:\n",
    "#     probability of p(y = 1 | x; w)\n",
    "def get_p(x, w):\n",
    "    wTx = 0.\n",
    "    for i, xi in x.items():\n",
    "        wTx += w[i] * xi  # w[i] * x[i]\n",
    "        \n",
    "    return 1. / (1. + exp(-max(min(wTx, 50.), -50.)))  # bounded sigmoid\n",
    "\n",
    "\n",
    "# D. Update given model\n",
    "# INPUT:\n",
    "#     w: weights\n",
    "#     n: a counter that counts the number of times we encounter a feature\n",
    "#        this is used for adaptive learning rate\n",
    "#     x: feature\n",
    "#     p: prediction of our model\n",
    "#     y: answer\n",
    "# OUTPUT:\n",
    "#     w: updated model\n",
    "#     n: updated count\n",
    "def update_w(w, g, x, p, y):\n",
    "    for i, xi in x.items():\n",
    "        g[i] += 1\n",
    "        w[i] -= (p - y) * xi * alpha / (sqrt(g[i]))  # Minimising log loss\n",
    "    return w, g\n",
    "\n",
    "\n",
    "# training and testing #######################################################\n",
    "\n",
    "# initialize our model\n",
    "w = [0.] * D  # weights\n",
    "g = [.5] * D  # sum of historical gradients\n",
    "\n",
    "# start training a logistic regression model using on pass sgd\n",
    "loss = 0.\n",
    "lossb = 0.\n",
    "for t, row in enumerate(DictReader(open(train),header, delimiter=',')):\n",
    "    y = 1. if row['Label'] == '1' else 0.\n",
    "\n",
    "    del row['Label']  # can't let the model peek the answer\n",
    "\n",
    "    # main training procedure\n",
    "    # step 1, get the hashed features\n",
    "    \n",
    "    x = get_x(row, D)\n",
    "    # step 2, get prediction\n",
    "    p = get_p(x, w)\n",
    "\n",
    "    # for progress validation, useless for learning our model\n",
    "    lossx = logloss(p, y)\n",
    "    loss += lossx\n",
    "    lossb += lossx\n",
    "    if t % logbatch == 0 and t > 1:\n",
    "        print('%s\\t Line : %d\\t logloss: %f\\t batch logloss: %f' % (datetime.now(), t, loss/t, lossb/logbatch))\n",
    "        lossb = 0.\n",
    "\n",
    "    # step 3, update model with answer\n",
    "    w, g = update_w(w, g, x, p, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pour alpha = 0.001\n",
    "2016-12-29 04:40:38.542845\t Line : 500000\t logloss: 0.549019\t batch logloss: 0.549019\n",
    "2016-12-29 04:42:38.738433\t Line : 1000000\t logloss: 0.539918\t batch logloss: 0.530816\n",
    "2016-12-29 04:44:46.764942\t Line : 1500000\t logloss: 0.533537\t batch logloss: 0.520776\n",
    "2016-12-29 04:46:44.721870\t Line : 2000000\t logloss: 0.528584\t batch logloss: 0.513725\n",
    "2016-12-29 04:48:38.084063\t Line : 2500000\t logloss: 0.525231\t batch logloss: 0.511818\n",
    "2016-12-29 04:50:35.794532\t Line : 3000000\t logloss: 0.523291\t batch logloss: 0.513592\n",
    "2016-12-29 04:52:36.770099\t Line : 3500000\t logloss: 0.521882\t batch logloss: 0.513427"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DumpObject(\"W_alpha2.1\", w)\n",
    "DumpObject(\"N_alpha2.1\", g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1041664"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.count(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version 2.2 1041657\n"
     ]
    }
   ],
   "source": [
    "print \"version 2.2\", w.count(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tar = tarfile.open(\"dac.tar.gz\")\n",
    "test=tar.extractfile('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DF = pd.read_csv(\"dac/test.txt\", header=-1, sep='\\t')\n",
    "DF = pd.DataFrame(DF)\n",
    "num_Cols=[]\n",
    "cat_Cols = []\n",
    "dd = {}\n",
    "for i in range(1,14):\n",
    "    dd[i]=\"I\"+`i`\n",
    "    num_Cols.append(\"I\"+`i`)\n",
    "for c in range(1,27):\n",
    "    dd[c+13]=\"C\"+`c`\n",
    "    cat_Cols.append(\"C\"+`c`)\n",
    "DF.rename(columns=dd, inplace=True)\n",
    "#%time cleanNull(DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C10:C7\n",
      "C10:C7:C11\n",
      "C10:C11\n",
      "C10:C11:C7\n",
      "C7:C10\n",
      "C7:C10:C11\n",
      "C7:C11\n",
      "C7:C11:C10\n",
      "C11:C10\n",
      "C11:C10:C7\n",
      "C11:C7\n",
      "C11:C7:C10\n",
      "CPU times: user 38.1 s, sys: 26.2 s, total: 1min 4s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "TOGEN = ['C10', 'C7' , 'C11']\n",
    "%time generateFeatureEngine(DF,TOGEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 6s, sys: 49.7 s, total: 5min 56s\n",
      "Wall time: 9min 13s\n"
     ]
    }
   ],
   "source": [
    "%time DF.to_csv(\"FinalTest.csv\" , header = False , index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# testing (build kaggle's submission file)\n",
    "with open('submission.csv', 'w') as submission:\n",
    "    submission.write('Id,Predicted\\n')\n",
    "    for t, row in enumerate(DictReader(open(\"FinalTest.csv\"), header[1:], delimiter=',')):\n",
    "        x = get_x(row, D)\n",
    "        p = get_p(x, w)\n",
    "        submission.write('%d,%f\\n' % (60000000+int(t), p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 8s, sys: 4.5 s, total: 5min 13s\n",
      "Wall time: 5min 20s\n"
     ]
    }
   ],
   "source": [
    "tar = tarfile.open(\"dac.tar.gz\")\n",
    "%time f = tar.extractfile('train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
